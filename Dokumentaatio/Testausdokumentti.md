Testing document
What has been tested and how
What types of input were used (especially important for comparative analysis)
How can the tests be repeated
Results of empirical testing presented in graphical form
Tests should ideally be a runnable program. This makes repeating the tests easy

Due to the nature of the project, I have opted out of unit testing. It is a bit of a shame, but when programming this kind of bot one usually runs the game everytime a potentially crashing change is made. This way once I ran into an error I could identify it easily and fix it. 

To some extent one can call running the game and looking for problems when things are happening in real time or going through the replays manually exploratory testing. This was done a lot.

The main and only testing is done on the replay files that are generated after every game is run. Through out this project I have generated a total of X replays. 

For presenting the final test results I have used the replays of the latest version of the bot and latest patch in the actual of sc2. 

I have divided the test results by replay cataloging each folder with different difficulties of default bots. 